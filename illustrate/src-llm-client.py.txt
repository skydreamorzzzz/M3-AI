一、文件核心定位
该文件是全系统唯一的 LLM/VLM 调用入口（统一模型客户端），作为所有模型调用的中枢层，核心作用如下：
为 Planner / Checker / Verifier 等模块提供统一的模型调用接口；
集成 SqliteCache 缓存机制，降低调用成本、保证实验可复现；
支持多种后端（mock /openai/gemini）灵活切换；
支持确定性运行（temperature=0 + keep-first 缓存策略）。
二、对外接口
该文件对外暴露 “参数结构 + 核心类” 两类接口，简洁且统一：
1. 参数结构
类名	核心作用
LLMParams	控制模型解码参数，包含 temperature /max_tokens/top_p /seed 等
2. 核心类与方法
类名	核心作用	核心方法	方法作用
LLMClient	统一模型客户端（唯一入口）	chat	通用模型调用入口，适配所有任务
三、统一调用流程
调用 LLMClient.chat (...) 方法时，执行以下固定流程，保证一致性与可复现性：
构造请求 payload：整合 messages（提示词） + images（图像，VLM 场景）；
生成稳定缓存 key：基于 task、模型信息、payload、参数生成唯一 cache key；
查询缓存：调用缓存层接口查询是否存在该 key 的缓存记录；
若命中缓存 → 直接返回缓存结果，跳过模型调用；
若未命中缓存 → 执行下一步；
调用后端模型：根据配置的后端（mock/openai/gemini），调用 _call_backend 执行真实 / 模拟调用；
写入缓存：将模型返回结果保存到缓存（默认不覆盖旧记录）；
返回结果：将模型输出文本返回给调用方。
四、支持的 task 类型
task 名称参与缓存 key 生成，保证不同任务的缓存互不干扰，推荐的标准化 task 命名如下：
extract_constraints：Planner 模块的约束抽取任务；
judge_constraint：Checker 模块的单约束判定任务；
verify_pair：Verifier 模块的两图对比验证任务；
general：通用文本交互任务（兜底）。
五、Mock 后端（骨架阶段核心）
当配置 backend="mock" 时，调用 _mock_response 生成模拟响应，核心特点与适配规则如下：
核心特点
必须返回 STRICT JSON（关键任务），保证下游解析不报错；
支持 deterministic 模式，输出结果固定；
用于 dry-run（空跑）、骨架调试，无真实模型调用成本。
各 task 的 mock 行为
task 名称	mock 响应规则
extract_constraints	根据 prompt 生成简单的结构化约束列表
judge_constraint	默认返回 passed=true（约束满足）
verify_pair	默认返回 decision="same"（两图效果相当）
核心目的
在无真实模型 / 不想调用模型时，让整个 pipeline 完整跑通，便于前期骨架调试。
六、_mock_extract_constraints 逻辑（Mock 核心子逻辑）
针对 extract_constraints 任务的 mock 生成规则，仅做基础结构化约束生成：
提取规则
从 prompt 中提取数字（支持 1-10 单词 + 数字）；
提取简单对象关键词（如 panda、cat、dog 等常见实体）；
提取简单风格词（如 watercolor、anime 等）。
生成约束类型
OBJECT 约束：必生成，对应提取的对象关键词；
COUNT 约束：若有数字则生成，无数字则跳过；
ATTRIBUTE 约束：若有风格词则生成（style 维度）。
输出格式
强制返回严格 JSON 结构：{"constraints": [...] }，保证下游解析兼容。
七、缓存机制整合
chat () 方法内部深度整合缓存逻辑，核心调用的工具 / 方法：
make_cache_key (...)：生成请求的唯一缓存 key；
self.cache.get (...)：查询缓存记录；
self.cache.set (...)：写入缓存记录。
默认缓存行为
采用 keep-first 策略（overwrite=False）：
若缓存中已存在该 key → 忽略写入，保留旧结果；
核心目的：保证实验可复现，相同请求始终返回相同结果。
八、各后端实现说明
1. OpenAI 后端（示例实现）
通过 _call_openai 方法实现，适配 OpenAI 接口特性：
若请求包含 images → 调用视觉模型接口（vision-style）；
若仅文本 → 调用普通 chat completion 接口；
仅做接口适配，不处理业务逻辑，直接返回模型原始文本输出。
2. Gemini 后端（骨架状态）
当前仅预留接口，未实现具体逻辑，调用时会抛出 NotImplementedError，支持后续扩展。
九、设计特点
单入口调用：所有 LLM/VLM 请求均通过 LLMClient.chat () 执行，统一管控；
解耦性强：与上层业务逻辑（Planner/Checker/Verifier）完全解耦，仅暴露统一接口；
内置缓存：无需上层模块关心缓存逻辑，客户端自动处理；
mock 独立运行：Mock 后端可让全流程脱离真实模型跑通，降低调试成本；
可扩展性：预留接口，可快速扩展新的模型后端（如 Claude、Qwen-VL 等）。
十、总结
该文件是整个 M3plus 系统的模型调度中心，是保障系统稳定性、可复现性的核心组件，核心职责：
统一所有 LLM/VLM 调用入口，避免接口碎片化；
内置缓存管理，降低实验成本、保证结果可复现；
负责后端分发（mock/openai/gemini），适配不同运行场景；
支持 mock 骨架运行，便于前期调试与流程验证。
所有系统内的 LLM/VLM 行为均需通过该客户端执行，是连接业务模块与模型服务的核心桥梁。